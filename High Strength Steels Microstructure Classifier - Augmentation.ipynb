{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microstructure Identification\n",
    "\n",
    "### High strength carbon steels microstructure classifier with convolutional neural networks\n",
    "\n",
    "Data Sourses: CMU UHCSDB http://uhcsdb.materials.cmu.edu/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils # label to categorical\n",
    "\n",
    "# read xlsx file for label and image path\n",
    "filename= './Input/micrograph.xlsx'\n",
    "dataread=pd.read_excel(filename,'micrograph',index_col=None, na_values=['NA'])\n",
    "head=dataread.columns.values.tolist()\n",
    "micro_id=dataread[\"micrograph_id\"].values.tolist()\n",
    "micro_constituents=dataread[\"primary_microconstituent\"].values.tolist()\n",
    "micro_paths=dataread[\"path\"].values.tolist()\n",
    "micro_files=[\"./Input/micrograph/\"+i[0:-4]+\"[cropped].tif\" for i in micro_paths]\n",
    "\n",
    "# encoding the label \n",
    "labelmap={'martensite':0,'network':1,'pearlite':2,'spheroidite':3,'pearlite+spheroidite':4,'pearlite+widmanstatten':5,'spheroidite+widmanstatten':6}\n",
    "micro_constituents_label = [labelmap[i] for i in micro_constituents]\n",
    "micro_targets=np_utils.to_categorical(np.array(micro_constituents_label), 7)\n",
    "\n",
    "# split files into training, validation, and test\n",
    "train_files=micro_files[0:700]\n",
    "valid_files=micro_files[700:850]\n",
    "test_files=micro_files[850:]\n",
    "\n",
    "train_targets=micro_targets[0:700]\n",
    "valid_targets=micro_targets[700:850]\n",
    "test_targets=micro_targets[850:]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total microstructure categories.' % len(labelmap))\n",
    "print('There are %s total microstructure images.\\n' % len(micro_targets))\n",
    "print('There are %d training microstructure images.' % len(train_targets))\n",
    "print('There are %d validation microstructure images.' % len(valid_targets))\n",
    "print('There are %d test microstructure images.'% len(test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(12):\n",
    "    ax = fig.add_subplot(2, 6, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(train_files[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CNN to Classify Microstructure (using Transfer Learning with Image Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D tensor as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $645 \\times 484$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 645, 484, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 645, 484, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def image_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(645,484))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (645, 484, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 645, 484, 3) and return 4D tensor\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return x\n",
    "\n",
    "def image_to_tensor(img_paths):\n",
    "    list_of_tensors = [feature_to_tensor(img_path,model) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def feature_to_tensor(img_path,model):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(645,484))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (645, 484, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 645, 484, 3) and return 4D tensor\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    return features\n",
    "\n",
    "def features_to_tensor(img_paths,model):\n",
    "    list_of_tensors = [feature_to_tensor(img_path,model) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=image_to_tensor(train_files)\n",
    "X_valid=image_to_tensor(valid_files)\n",
    "X_test =image_to_tensor(test_files)\n",
    "\n",
    "# print shape of training set\n",
    "print('x_train shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Create and Configure Augmented Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create and configure augmented image generator\n",
    "datagen_train = ImageDataGenerator(\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n",
    "    horizontal_flip=True) # randomly flip images horizontally\n",
    "\n",
    "# create and configure augmented image generator\n",
    "datagen_valid = ImageDataGenerator(\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n",
    "    horizontal_flip=True) # randomly flip images horizontally\n",
    "\n",
    "# fit augmented image generator on data\n",
    "datagen_train.fit(X_train)\n",
    "datagen_valid.fit(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Original and Augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# take subset of training data\n",
    "x_train_subset = X_train[:12]\n",
    "\n",
    "# visualize subset of training data\n",
    "fig = plt.figure(figsize=(20,2))\n",
    "for i in range(0, len(x_train_subset)):\n",
    "    ax = fig.add_subplot(1, 12, i+1)\n",
    "    ax.imshow(x_train_subset[i])\n",
    "fig.suptitle('Subset of Original Training Images', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# visualize augmented images\n",
    "fig = plt.figure(figsize=(20,2))\n",
    "for x_batch in datagen.flow(x_train_subset, batch_size=12):\n",
    "    for i in range(0, 12):\n",
    "        ax = fig.add_subplot(1, 12, i+1)\n",
    "        ax.imshow(x_batch[i])\n",
    "    fig.suptitle('Augmented Images', fontsize=20)\n",
    "    plt.show()\n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Obtain Bottleneck Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "train_ResNet50 = features_to_tensor(train_files,base_model)\n",
    "valid_ResNet50  = features_to_tensor(valid_files,base_model)\n",
    "test_ResNet50  = features_to_tensor(test_files,base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Model Architecture\n",
    "\n",
    "The model uses the the pre-trained ResNet50 model as a fixed feature extractor, where the last convolutional output of ResNet50 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "ResNet50_model = Sequential()\n",
    "ResNet50_model.add(GlobalAveragePooling2D(input_shape=train_ResNet50.shape[1:]))\n",
    "ResNet50_model.add(Dense(7, activation='softmax'))\n",
    "ResNet50_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: Compile the Model and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResNet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.ResNet50.Aug.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "# hist = ResNet50_model.fit_generator(train_ResNet50, train_targets, \n",
    "#           validation_data=(valid_ResNet50, valid_targets),\n",
    "#           epochs=100, batch_size=20, callbacks=[checkpointer], verbose=0)\n",
    "batch_size=20\n",
    "\n",
    "hist = ResNet50_model.fit(train_ResNet50, train_targets, \n",
    "          validation_data=(valid_ResNet50, valid_targets),\n",
    "          epochs=100, batch_size=20, callbacks=[checkpointer], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6: Load the Model with the Best Validation Loss and Test the accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResNet50_model.load_weights('saved_models/weights.best.ResNet50.Aug.hdf5')\n",
    "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
